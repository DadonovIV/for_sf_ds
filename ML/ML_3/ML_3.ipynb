{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжаем знакомство с моделями МО в области обучения с учителем. На этот раз поговорим о задаче **классификации**. Вспомним, где находится классификация на нашей карте машинного обучения:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml1-3_2 (1).png)\n",
    "\n",
    "Вначале мы снова обратимся к классу линейных моделей и рассмотрим **логистическую регрессию**.\n",
    "\n",
    "↓\n",
    "\n",
    "Затем поговорим о **деревьях решений** для задачи классификации и научимся строить из этих деревьев целый лес.\n",
    "\n",
    "**Цели данного модуля:**\n",
    "\n",
    "* Познакомиться с принципами работы модели логистической регрессии для решения задачи классификации. \n",
    "* Рассмотреть метрики классификации и научиться оценивать качество моделей, решающих данную задачу.\n",
    "* Узнать принципы построения деревьев решений.\n",
    "* Изучить основы ансамблевых моделей типа бэггинг на примере случайного леса.\n",
    "* Научиться применять деревья решений и случайные леса для решения задачи классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`✍ Ранее мы обсуждали модель линейной регрессии, которая предназначена для решения задачи регрессии. Теперь нам предстоит разобраться с тем, как преобразовать данную модель, чтобы она решала задачу классификации.`\n",
    "\n",
    "Для начала вспомним, что такое классификация.\n",
    "\n",
    "**Задача классификации (classification)** *— задача, в которой мы пытаемся предсказать класс объекта на основе признаков в наборе данных. То есть задача сводится к предсказанию целевого признака, который является **категориальным**.*\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml1-3_9.png)\n",
    "\n",
    "Когда классов, которые мы хотим предсказать, только два, классификация называется **бинарной**. Например, мы можем предсказать, болен ли пациент раком, является ли изображение человеческим лицом, является ли письмо спамом и т. д.\n",
    "\n",
    "Когда классов, которые мы хотим предсказать, более двух, классификация называется **мультиклассовой (многоклассовой)**. Например, предсказание модели самолёта по радиолокационным снимкам, классификация животных на фотографиях, определение языка, на котором говорит пользователь, разделение писем на группы.\n",
    "\n",
    "    → Для простоты мы пока разберёмся с бинарной классификацией, а в следующем юните обобщим результат на мультиклассовую.\n",
    "\n",
    "Что вообще означает «решить задачу классификации»? Это значит построить разделяющую поверхность в пространстве признаков, которая делит пространство на части, каждая из которых соответствует определённому классу. \n",
    "\n",
    "Ниже представлены примеры разделяющих поверхностей, которые производят бинарную классификацию. Красным и синим цветом обозначены классы, зелёным — собственно поверхность, которая делит пространство признаков на две части. В каждой из этих частей находятся только наблюдения определённого класса.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_1.png)\n",
    "\n",
    "*Модели, которые решают задачу классификации, называются **классификаторами (classifier)**.*\n",
    "\n",
    "Если взять в качестве разделяющей поверхности некоторую плоскость (ровная поверхность на первом рисунке), то мы получаем модель логистической регрессии, которая тесно связана с рассмотренной нами ранее линейной регрессией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для начала вспомним, как выглядит уравнение модели линейной регрессии в общем случае:\n",
    "\n",
    "![](./image/2024-08-01_20-59-15.png)\n",
    "\n",
    "В общем случае это уравнение гиперплоскости, которая стремится приблизить зависимость целевой переменной от $m$ факторов.\n",
    "\n",
    "* Когда фактор всего один, уравнение задаёт прямую:\n",
    "\n",
    "$y^` = w_0 + w_1x$\n",
    "\n",
    "* Когда факторов два, уравнение задаёт плоскость:\n",
    "\n",
    "$y^`=w_0+w_1x_1+w_2x_2$\n",
    "\n",
    "→ Но всё это работает только в том случае, когда целевой признак $y$, который мы хотим предсказать, является числовым, например цена, вес, время аренды и т. д.\n",
    "\n",
    "Что же делать с этой моделью, когда целевой признак $y$ является категориальным? Например, является письмо спамом или обычным письмом?\n",
    "\n",
    "Можно предположить, что, раз у нас есть две категории, мы можем обозначить категории за $y=1$ (Спам) и $y=0$ (Не спам) и обучить линейную регрессию предсказывать 0 и 1.\n",
    "\n",
    "Но результат будет очень плохим. Выглядеть это будет примерно так:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_2.png)\n",
    "\n",
    "Для больших значений $x$ прямая будет выдавать значения больше 1, а для очень маленьких — меньше 0. Что это значит? Непонятно. Непонятно и то, что делать со значениями в диапазоне от 0 до 1. Да, можно относить значения на прямой выше 0.5 к классу 1, а меньше либо равным 0.5 — к классу 0, но это всё «костыли».\n",
    "\n",
    "`Идея! Давайте переведём задачу классификации в задачу регрессии. Вместо предсказания класса будем предсказывать вероятность принадлежности к этому классу.` \n",
    "\n",
    "Модель должна выдавать некоторую вероятность $P$, которая будет определять, принадлежит ли данный объект к классу 1: например, вероятность того, что письмо является спамом. При этом вероятность того, что письмо является обычным письмом (класс 0), определяется как $Q=1-P$.  \n",
    "\n",
    "Когда модель будет обучена на предсказание вероятности, мы зададим некоторый порог вероятности. Если предсказанная вероятность будет выше этого порога, мы определим объект к классу 1, а если ниже — к классу 0.\n",
    "\n",
    "Например, стандартный порог равен 0.5. То есть если вероятность $P>0.5$, мы будем считать письмо спамом, а если $P<=0.5$ — обычным информативным письмом.\n",
    "\n",
    "В итоге мы добьёмся того, что будем предсказывать не дискретный категориальный, а непрерывный числовой признак, который лежит в диапазоне [0, 1]. А это уже знакомая нам задача регрессии.\n",
    "\n",
    "    → Однако остался главный вопрос: как научить модель предсказывать вероятности, ведь они должны лежать строго в диапазоне от 0 до 1, а предсказания линейной регрессии лежат в диапазоне от $-infinity$ до $+infinity$? \n",
    "\n",
    "Тут-то мы и приходим к модели логистической регрессии — **регрессии вероятностей**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "**Логистическая регрессия (Logistic Regression)** *— одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в Data Science.*\n",
    "\n",
    "В основе логистической регрессии лежит логистическая функция (*logistic function*) $sigma(z)$ — отсюда и название модели. Однако более распространённое название этой функции — **сигмόида (sigmoid)**. Записывается она следующим образом:\n",
    "\n",
    "![](./image/2024-08-01_21-07-45.png)\n",
    "\n",
    "Примечание. Здесь $e$ — экспонента или число Эйлера. Это число является бесконечным, а его значение обычно принимают равным $2.718...$.\n",
    "\n",
    "А вот график зависимости сигмоиды от аргумента $z$:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_3.png)\n",
    "\n",
    "**В чём преимущество этой функции?**\n",
    "\n",
    "*У сигмоиды есть два очень важных для нас свойства:*\n",
    "\n",
    "* Значения сигмоиды $sigma(z)$ лежат в диапазоне от 0 до 1 при любых значения аргумента $z$: какой бы $z$ вы ни подставили, число меньше 0 или больше 1 вы не получите.\n",
    "* Сигмоида выдаёт значения $sigma(z)>0.5$ при её аргументе $x>0$, $sigma(z)<0.5$ — при $z<0 и $sigma(z)=0.5$ — при $z=0$.\n",
    "\n",
    "Это ведь и есть свойства вероятности! Выходом сигмоиды является число от 0 до 1, которое можно интерпретировать как вероятность принадлежности к классу 1. Её мы и пытаемся предсказать.\n",
    "\n",
    "**Основная идея** модели логистической регрессии: возьмём модель линейной регрессии (обозначим её выход за $z$)\n",
    "\n",
    "![](./image/2024-08-01_21-17-21.png)\n",
    "\n",
    "и подставим выход модели $z$ в функцию сигмоиды, чтобы получить искомые оценки вероятности (в математике принято писать оценочные величины с «шапкой» наверху, а истинные значения — без «шапки», это чистая формальность):\n",
    "\n",
    "![](./image/2024-08-01_21-18-54.png)\n",
    "\n",
    "**Примечание.** *Далее в модуле мы будем называть оценки вероятности $P^`$ просто вероятностью, но только для краткости. Это не значит, что эти оценки являются истинными вероятностями принадлежности к каждому из классов (их нельзя сосчитать, так как для этого нужна выборка бесконечного объёма). Если вы употребляете термин «вероятности» на собеседованиях, обязательно предварительно укажите, что вы подразумеваете оценку вероятности.*\n",
    "\n",
    "Обучать будем всё в совокупности, пытаясь получить наилучшую оценку вероятности $P^`$. Если вероятность $P^`>0.5$, относим объект к классу 1, а если $P^`<=0.5$, относим объект к классу 0. \n",
    "\n",
    "Математически это записывается следующей формулой:\n",
    "\n",
    "![](./image/2024-08-01_21-21-56.png)\n",
    "\n",
    "**Примечание.** *В данном выражении $I[P^`]$ называется **индикаторной функцией**. Она возвращает 1, если её значение больше какого-то порога, и 0 — в противном случае. Математики часто записывают просто квадратные скобки, опуская символ $I$: $[P^`]$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чего мы добились таким преобразованием?**\n",
    "\n",
    "Если мы обучим модель, то есть подберём  коэффициенты $w_0, w_1, w_2, ..., w_m$ (как их найти, обсудим чуть позже) таким образом, что для объектов класса 1 модель линейной регрессии начнёт выдавать положительное число, а для класса 0 — выдавать отрицательное число, то тогда, подставив предсказание линейной регрессии $z$ в сигмоиду, мы сможем получать вероятности принадлежности к каждому из классов в диапазоне от 0 до 1.\n",
    "\n",
    "Далее по порогу вероятности мы сможем определять, к какому классу принадлежит объект.\n",
    "\n",
    "Это и есть наша цель. Мы свели задачу классификации к задаче регрессии для предсказания вероятностей. \n",
    "\n",
    "Для бинарной классификации описанное выше будет выглядеть следующим образом:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_4.png)\n",
    "\n",
    "![](./image/2024-08-01_21-26-21.png)\n",
    "\n",
    "**Разберёмся с геометрией**\n",
    "\n",
    "Возьмём частный случай, когда класс объекта зависит от двух признаков — $x_1$ и $x_2$.\n",
    "\n",
    "![](./image/2024-08-01_21-31-25.png)\n",
    "\n",
    "![](./image/2024-08-01_21-32-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В чём математический секрет?**\n",
    "\n",
    "Математически подстановка в уравнение плоскости точки, которая не принадлежит ей (находится ниже или выше), означает вычисление расстояния от этой точки до плоскости.\n",
    "\n",
    "* Если точка находится ниже плоскости, расстояние будет отрицательным ($z<0$).\n",
    "* Если точка находится выше плоскости, расстояние будет положительным ($z>0$).\n",
    "* Если точка находится на самой плоскости, $z=0$.\n",
    "\n",
    "Мы знаем, что подстановка отрицательных чисел в сигмоиду приведёт к вероятности $P^`<0.5$, а постановка положительных — к вероятности $P^`>0.5$. \n",
    "\n",
    "*Таким образом, ключевым моментом в предсказании логистической регрессии является расстояние от точки до разделяющей плоскости в пространстве факторов. Это расстояние в литературе часто называется* **отступом (margin)**. \n",
    "\n",
    "В этом и состоит секрет работы логистической регрессии.\n",
    "\n",
    "*Чем больше расстояние от точки, находящейся выше разделяющей плоскости, до самой плоскости, тем больше оценка вероятности принадлежности к классу 1.*\n",
    "\n",
    "Попробуйте подставить различные координаты точек в модель логистической регрессии и убедитесь в этом.\n",
    "\n",
    "Можно построить тепловую карту, которая показывает, чему равны вероятности в каждой точке пространства:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_7.png)\n",
    "\n",
    "`На рисунке точки, которые относятся к классу непоступивших абитуриентов, лежащие ниже разделяющей плоскости, находятся в красной зоне. Чем насыщеннее красный цвет, тем ниже вероятность того, что абитуриент поступит в аспирантуру. И наоборот, точки, которые относятся к классу поступивших абитуриентов, лежащие выше разделяющей плоскости, находятся в синей зоне. Чем насыщеннее синий цвет, тем выше вероятность того, что абитуриент поступит в аспирантуру.`\n",
    "\n",
    "Для случая зависимости целевого признака $y$ от трёх факторов $x_1$, $x_2$ и $x_3$, например от баллов за два экзамена и рейтинга университета, из которого выпустился абитуриент, выражение для $z$ будет иметь вид:\n",
    "\n",
    "$z=w_0+w_1x_1+w_2x_2+w_3x_3$\n",
    "\n",
    "Уравнение задаёт плоскость в четырёхмерном пространстве. Но если вспомнить, что $y$ — категориальный признак и классы можно обозначить цветом, то получится перейти в трёхмерное пространство. Разделяющая плоскость будет выглядеть следующим образом:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_8.png)\n",
    "\n",
    "В общем случае, когда у нас есть зависимость от $m$ факторов, линейное выражение, находящееся под сигмоидой, будет обозначать **разделяющую гиперплоскость**.\n",
    "\n",
    "![](./image/2024-08-01_21-39-41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> ПОИСК ПАРАМЕТРОВ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "Итак, мы разобрались с тем, как выглядит модель логистической регрессии и что она означает в геометрическом смысле.\n",
    "\n",
    "Но остался один главный вопрос: как найти такие коэффициенты $w=(w_0, w_1, w_2, ..., w_m)$, чтобы гиперплоскость разделяла пространство наилучшим образом?\n",
    "\n",
    "Вновь обратимся к нашей схеме минимизации эмпирического риска:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml1-2_1.png)\n",
    "\n",
    "Можно предположить, что стоит использовать метод наименьших квадратов. Введём функцию ошибки — средний квадрат разности MSE между истинными классами $y$ и предсказанными классами $y^`$ и попытаемся его минимизировать.\n",
    "\n",
    "Сразу можно достоверно предсказать, что результат такого решения будет плохим, поэтому воздержимся от его использования.\n",
    "\n",
    "Здесь нужен другой подход. Это **метод максимального правдоподобия (Maximum Likelihood Estimation — MLE)**. \n",
    "\n",
    "**Правдоподобие** *— это оценка того, насколько вероятно получить истинное значение целевой переменной  при данных  и параметрах $w$.*\n",
    "\n",
    "Данный метод позволяет получить функцию правдоподобия.\n",
    "\n",
    "**Цель метода** *— найти такие параметры $w=(w_0, w_1, w_2, ..., w_m)$, в которых наблюдается максимум функции правдоподобия.*\n",
    "\n",
    "А мы пока что опустим математические детали метода и приведём только конечную формулу:\n",
    "\n",
    "![](./image/2024-08-01_21-45-58.png)\n",
    "\n",
    "Не пугайтесь. Давайте разберёмся, что есть что и как работает эта функция.\n",
    "\n",
    "* $n$ — количество наблюдений.\n",
    "* $y_i$ — это истинный класс (1 или 0) для -ого объекта из набора данных.\n",
    "* $P^`=sigma(z_i)$ — предсказанная с помощью логистической регрессии вероятность принадлежности к классу 1 для $i$-ого объекта из набора данных.\n",
    "* $z_i$ — результат подстановки -ого объекта из набора данных в уравнение разделяющей плоскости $z_i=w^`*x_i^`$.\n",
    "* $log$ — логарифм (обычно используется натуральный логарифм по основанию $e-ln$).\n",
    "\n",
    "![](./image/2024-08-01_21-54-39.png)\n",
    "\n",
    "Такие расчёты можно производить для любых значений параметров, меняется только оценка вероятности $P_i^`$.\n",
    "\n",
    "**Примечание.** *К сожалению, функция likelihood не имеет интерпретации, то есть нельзя сказать, что значит число -2.34 в контексте правдоподобия.*\n",
    "\n",
    "Цель — найти такие параметры, при которых наблюдается максимум этой функции.\n",
    "\n",
    "Теперь пора снова применить магию математики, чтобы привести задачу к привычному нам формату минимизации эмпирического риска. По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь $L(w)$, которая носит название **«функция логистических потерь»**, или **logloss**. Также часто можно встретить название **кросс-энтропия**, или **cross-entropy loss**:\n",
    "\n",
    "![](./image/2024-08-01_21-56-49.png)\n",
    "\n",
    "Вот эту функцию мы и будем минимизировать в рамках поиска параметров логистической регрессии. Мы должны найти такие параметры разделяющей плоскости $w$, при которых наблюдается минимум logloss.\n",
    "\n",
    "Знакомая задача? Всё то же самое, что и с линейной регрессией, только функция ошибки другая.\n",
    "\n",
    "    → К сожалению, для такой функции потерь аналитическое решение оптимизационной задачи найти не получится: при расчётах получается, что его попросту не существует.\n",
    "\n",
    "Но мы помним, что, помимо аналитических решений, есть и численные.\n",
    "\n",
    "Например, для поиска параметров можно использовать знакомый нам градиентный спуск. Вспомним, как выглядит итерационная формула данного метода:\n",
    "\n",
    "![](./image/2024-08-01_21-58-06.png)\n",
    "\n",
    "Повторим её смысл: новое значение параметров $w^{(k+1)}$ получается путём сдвига текущих $w^{(k)}$ в сторону вектора антиградиента $Grad L(w^{(k)})$, умноженного на темп обучения $nu$.\n",
    "\n",
    "    Мы уже знаем, что для того, чтобы повысить шанс пройти мимо локальных минимумов функции потерь, используется не сам градиентный спуск, а его модификации: например, можно использовать уже знакомый нам стохастический градиентный спуск (SGD).\n",
    "\n",
    "`Помним, что применение градиентного спуска требует предварительного масштабирования данных (стандартизации/нормализации). В реализации логистической регрессии в sklearn предусмотрено ещё несколько методов оптимизации, для которых масштабирование не обязательно. О них мы упомянем в практической части модуля.`\n",
    "\n",
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется **регуляризация**. В реализации логистической регрессии в *sklearn* она немного отличается от той, что мы видели ранее для линейной регрессии.\n",
    "\n",
    "При L1-регуляризации мы добавляем в функцию потерь $L(w)$ штраф из суммы модулей параметров, а саму функцию logloss умножаем на коэффициент $C$:\n",
    "\n",
    "![](./image/2024-08-01_22-10-48.png)\n",
    "\n",
    "Значение коэффициента $C$ — коэффициент, обратный коэффициенту регуляризации. Чем **больше** $C$, тем **меньше** «сила» регуляризации.\n",
    "\n",
    "Предлагаем вам посмотреть на то, как будет меняться форма сигмоиды, разделяющей плоскости при минимизации функции потерь logloss (она обозначена как cross-entropy в виде концентрических кругов — вид сверху), с помощью обычного градиентного спуска (не стохастического) в виде анимации.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_12.gif)\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_13.gif)\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_14.gif)\n",
    "\n",
    "Не волнуйтесь, все громоздкие формулы уже реализованы в классических библиотеках, таких как sklearn. Но нам важно понимать принцип того, что происходит «под капотом», чтобы верно интерпретировать результат и по возможности управлять им."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ В SKLEARN\n",
    "\n",
    "    Мы будем работать со знакомым вам по модулю «Очистка данных» набором данных о диабете, первоначально полученным в Национальном институте диабета, болезней органов пищеварения и почек.\n",
    "\n",
    "    Наша цель будет состоять в том, чтобы диагностически предсказать, есть ли у пациента диабет. На выбор экземпляров из более крупной базы данных было наложено несколько ограничений. В частности, все пациенты здесь — женщины не моложе 21 года из индейского племени Пима.\n",
    "\n",
    "В модуле по очистке данных вы уже производили очистку этого набора данных:\n",
    "\n",
    "* удалили дубликаты,\n",
    "* удалили неинформативный признак Gender,\n",
    "* обработали «скрытые» пропуски в данных,\n",
    "* избавились от потенциальных выбросов.\n",
    "\n",
    "#### Результат представлен в ./Логистическая_регрессия.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ✍ Мы научились обучать модель логистической регрессии, чтобы решать с её помощью задачу классификации. Теперь мы познакомимся с метриками классификации, чтобы научиться оценивать качество модели.\n",
    "\n",
    "Однако прежде чем мы перейдём к этим метрикам, нам очень важно освежить в памяти некоторую терминологию из статистики, а именно вспомнить, что такое ошибки I и II рода.\n",
    "\n",
    "### <center>ОШИБКИ I И II РОДА С ТОЧКИ ЗРЕНИЯ КЛАССИФИКАЦИИ\n",
    "\n",
    "Давайте рассмотрим предсказания алгоритма $y_i^`$ на конкретном объекте $x_i$ под номером из данных $i$ с точки зрения статистических гипотез.\n",
    "\n",
    "Будем считать класс 1 (диабет есть) положительным исходом (Positive), а класс 0 (диабета нет) — отрицательным (Negative).\n",
    "\n",
    "**Примечание.** *На первый взгляд такая терминология может показаться не совсем этичной, так как наличие диабета — это всё-таки отрицательный сценарий для пациента. Однако для унификации терминологии в машинном обучении в большинстве задач объекты класса 1 считаются объектами с наличием некоторого эффекта (болезнь есть / задолженность погашена / клиент ушёл / устройство отказало и т. д.), а объекты класса 0 — объектами с отсутствием этого эффекта (болезни нет / задолженность не погашена / клиент не ушёл / устройство работает без отказов и т. д.).*\n",
    "\n",
    "Пусть у нас есть некоторый пациент $x_i$, и мы хотим понять, болен ли он диабетом. С точки зрения задачи классификации мы хотим предсказать истинный класс ($y_i$) пациента.\n",
    "\n",
    "Нулевая гипотеза будет состоять в отсутствии эффекта (пациент не болен диабетом), то есть , а альтернативная — в его наличии (пациент болен диабетом) $y_i=0$, то есть $y_i=1$. В терминах статистических гипотез это будет записано так:\n",
    "\n",
    "* $H_0$: Пациент $x_i$ не болеет диабетом $y_i=0$.\n",
    "* $H_1$: Пациент $x_i$ болеет диабетом $y_i=1$.\n",
    "Тогда у нас есть два случая, в которых мы можем допустить ошибку:\n",
    "\n",
    "Ошибка I (первого) рода ($alpfa$-ошибка): отклонение нулевой гипотезы, когда она на самом деле верна, или **ложноположительный результат**. То есть мы предсказали, что пациент болен диабетом, хотя это не так.\n",
    "Ошибка II (второго) рода ($betta$-ошибка): принятие нулевой гипотезы, когда она на самом деле ложна, или **ложноотрицательный результат**. То есть мы предсказали, что пациент здоров, хотя на самом деле он болен диабетом.\n",
    "\n",
    "**Примечание.** *Как вы можете понять, в диагностических задачах для нас критичнее ошибка II рода. Последствия будут более серьёзными, если мы примем больного пациента за здорового, чем если мы примем здорового за больного. Нам важно охватить всех потенциально больных пациентов, чтобы сделать дополнительный анализ и удостовериться в результате.*\n",
    "\n",
    "    Отлично, мы освежили в памяти ошибки I и II рода — это поможет нам понять суть метрик классификации. Давайте перейдём к ним."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>МЕТРИКИ КЛАССИФИКАЦИИ\n",
    "\n",
    "Будем рассматривать метрики для задачи классификации на следующем примере.\n",
    "\n",
    "Мы случайным образом выбрали десять пациентов из нашей таблицы и моделью log_reg_full предсказали для них ответы:\n",
    "\n",
    "![](./image/ML_3_1.png)\n",
    "\n",
    "Все метрики, которые мы рассмотрим, основаны на матрице ошибок. С неё мы и начнём наш разбор.\n",
    "\n",
    "1. **Матрица ошибок (confusion matrix)** *показывает все возможные исходы совпадения и несовпадения предсказания модели с действительностью. Используется для расчёта других метрик.*\n",
    "\n",
    "Допустим, что у нас есть два класса и алгоритм, предсказывающий принадлежность каждого объекта к одному из классов. Тогда каждая ячейка матрицы ошибок соответствует количеству объектов, попавших в одну из следующих четырёх категорий:\n",
    "\n",
    "* **Истинно положительные (True Positive, TP)** — это объекты, обозначенные моделью как класс 1 ($y^`=1$) и действительно принадлежащие к классу 1 ($y=1$).\n",
    "* **Ложноположительные (False Positive, FP)** — это объекты, обозначенные моделью как класс 1 ($y^`=1$), но в действительности принадлежащие к классу 0 ($y=0$). То есть это объекты, для которых модель совершила ошибку I рода.\n",
    "* **Истинно отрицательные (True Negative, TN)** — это объекты, обозначенные моделью как класс 0 ($y^`=0$) и действительно принадлежащие к классу 0 ($y=0$).\n",
    "* **Ложноотрицательные (False Negative, FN)** — это объекты, обозначенные моделью как класс 0 ($y^`=0$), но в действительности принадлежащие к классу 1 ($y=1$). То есть это объекты, для которых модель совершила ошибку II рода.\n",
    "\n",
    "Общий вид матрицы ошибок будет следующим:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_3_1.png)\n",
    "\n",
    "Давайте посмотрим, как будет выглядеть матрица ошибок для нашего примера предсказаний модели *log_reg_full*:\n",
    "\n",
    "![](./image/ML_3_2.png)\n",
    "\n",
    "Для большей наглядности представим следующую ситуацию: у нас есть множество наблюдений двух классов (класс 1 — круги, класс 0 — квадраты). Пусть мы нарисовали некоторый круг и условились, что все объекты, лежащие в этом круге, мы будем считать классом 1, а объекты вне круга — классом 0. Тогда мы получим следующую картину:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_3_2.png)\n",
    "\n",
    "Таким образом, мы получили четыре группы объектов:\n",
    "\n",
    "* Объекты, попавшие в круг, для которых мы предсказали класс 1, и эти точки действительно относятся к классу 1 — *True Positive (TP)*.\n",
    "\n",
    "* Объекты, попавшие в круг, для которых мы предсказали класс 1, но эти точки относятся к классу 0 — *False Positive (FP)*.\n",
    "\n",
    "* Объекты за пределами круга, для которых мы предсказали класс 0, и эти точки действительно относятся к классу 0 — *True Negative (TN)*.\n",
    "\n",
    "* Объекты за пределами круга, для которых мы предсказали класс 0, но эти точки относятся к классу 1 — *False Negative (FN)*.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_3_3.png)\n",
    "\n",
    "Формально матрица ошибок не является метрикой, но на её основе составляются сами метрики классификации. Давайте перейдём к ним.\n",
    "\n",
    "2. **Accuracy (достоверность/аккуратность)** *— доля правильных ответов модели среди всех ответов. Правильные ответы — это истинно положительные (True Positive) и истинно отрицательные ответы (True Negative):*\n",
    "\n",
    "![](./image/ML_3_3.png)\n",
    "\n",
    "**Примечание.** Нередко в русской литературе вы можете встретить перевод метрики *accuracy* как «точность», однако так же на русский язык переводится метрика *precision*, о которой мы поговорим далее. Поэтому, если вы используете термин «точность», старайтесь указывать, о какой именно метрике (*accuracy* или *precision*) идёт речь.\n",
    "\n",
    "В виде диаграммы соотношение количества объектов, классы которых мы угадали и общего количества объектов записывается в следующем виде:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_3_4.png)\n",
    "\n",
    "**Интерпретация:** *как много (в долях) модель угадала ответов.*\n",
    "\n",
    "Метрика изменяется в диапазоне от 0 до 1. Чем ближе значение к 1, тем больше ответов модель «угадала». \n",
    "\n",
    "Рассчитаем accuracy для нашего примера:\n",
    "\n",
    "![](./image/ML_3_4.png)\n",
    "\n",
    "Итак, наша accuracy равна 0.5, то есть модель сделала верное предсказание для 50 % пациентов из выборки.\n",
    "\n",
    "`Accuracy — самая простая и самая понятная метрика классификации, но у неё есть один существенный недостаток. Она бесполезна, если классы сильно несбалансированы. Продемонстрируем это следующим примером.`\n",
    "\n",
    "    Допустим, мы хотим оценить работу спам-фильтра электронной почты. Спам-письма обозначены как Positive (1), а не спам — как Negative (0).\n",
    "\n",
    "У нас есть 110 писем. Из них:\n",
    "\n",
    "* 100 писем — не спам,\n",
    "* 10 писем — спам.\n",
    "\n",
    "Мы построили некоторый классификатор. Согласно результатам классификатора, из 100 писем класса «не спам» мы верно определили (отнесли к классу 0) 90, а оставшиеся 10 отнесли к классу «спам» (классу 1). То есть True Negative = 90, а False Positive = 10. Из 10 спам-писем мы верно определили 5 (отнесли к классу 1), а остальные 5 отнесли к классу «не спам» (классу 0). То есть True Positive = 5, а False Negative = 5.\n",
    "\n",
    "Матрица ошибок будет иметь вид:\n",
    "\n",
    "![](./image/ML_3_5.png)\n",
    "\n",
    "Тогда *accuracy*:\n",
    "\n",
    "![](./image/ML_3_6.png)\n",
    "\n",
    "Однако представим, что мы построили классификатор, который просто предсказывает все письма как «не спам», то есть True Negative = 100, False Negative = 10, True Positive = 0, False Positive = 0.\n",
    "\n",
    "Матрица ошибок будет иметь вид:\n",
    "\n",
    "![](./image/ML_3_7.png)\n",
    "\n",
    "Тогда *accuracy* будет равна:\n",
    "\n",
    "![](./image/ML_3_8.png)\n",
    "\n",
    "    Метрика выросла, однако наша «модель» не обладает никакой предсказательной силой, так как изначально мы хотели определять письма со спамом. Преодолеть это нам поможет переход с общей для всех классов метрики к отдельным показателям качества классов, а именно к метрикам precision, recall и -мера.\n",
    "\n",
    "3. **Precision (точность), или PPV (Positive Predictive Value)** *— это доля объектов, названных классификатором положительными и при этом действительно являющихся таковыми, по отношению ко всем названным положительными объектам.*\n",
    "\n",
    "![](./image/ML_3_9.png)\n",
    "\n",
    "В виде диаграммы соотношение количества объектов класса 1, которые мы угадали и количества объектов, которые мы приняли за класс 1, записывается следующим образом:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_3_5.png)\n",
    "\n",
    "Метрика также изменяется от 0 до 1. \n",
    "\n",
    "**Интерпретация:** способность отделить класс 1 от класса 0. Чем больше *precision*, тем меньше ложных попаданий. **То есть чем ближе *precision* к 1, тем меньше вероятность модели допустить ошибку I рода.**\n",
    "\n",
    "Именно *precision* не позволяет записать все ответы в один класс, так как в таком случае резко возрастает значение *False Positive* и метрика снижается.\n",
    "\n",
    "Рассчитаем *precision* для нашего примера:\n",
    "\n",
    "![](./image/ML_3_10.png)\n",
    "\n",
    "Таким образом, количество названных моделью больных диабетом и при этом действительно являющихся больными составляет 67 % от всех пациентов.\n",
    "\n",
    "Precision нужен в задачах, где от нас требуется минимум ложных срабатываний. Чем выше «цена» ложноположительного результата, тем выше должен быть *precision*.\n",
    "\n",
    "Например, по камерам видеонаблюдения мы автоматически выявляем признаки драки на улицах и отправляем наряд полиции для урегулирования конфликта. Однако штат сотрудников сильно ограничен, реагировать на любой признак конфликта мы не можем, поэтому мы хотим уменьшить количество ложных вызовов. В такой ситуации мы выберем такую модель, у которой наибольший *precision*.\n",
    "\n",
    "В предельном случае (когда *precision* равен 1) у модели отсутствуют ложноположительные срабатывания.\n",
    "\n",
    "**Примечание.** *Важно понимать, что данный вывод справедлив только для выборки, на которой мы оцениваем метрику, то есть это не означает, что модель вовсе не может допустить ложноположительных результатов. Однако чем больше выборка, на которой мы тестируем алгоритм, тем ближе к истине будет данный вывод.*\n",
    "\n",
    "4. **Recall (полнота), или TPR (True Positive Rate)** *— это доля объектов, названных классификатором положительными и при этом действительно являющихся таковыми, по отношению ко всем объектам положительного класса.*\n",
    "\n",
    "![](./image/ML_3_11.png)\n",
    "\n",
    "Диаграмма:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_3_6.png)\n",
    "\n",
    "Метрика изменяется от 0 до 1.\n",
    "\n",
    "**Интерпретация:** способность модели обнаруживать класс 1 вообще, то есть охват класса 1. Заметьте, что метрика зависит от количества ложноотрицательных срабатываний. **То есть чем ближе recall к 1, тем меньше вероятность модели допустить ошибку II рода.**\n",
    "\n",
    "Рассчитаем *recall* для нашего примера:\n",
    "\n",
    "![](./image/ML_3_12.png)\n",
    "\n",
    "Итак, процент пациентов, которых модель определила к классу больных диабетом, среди всех действительно больных диабетом составляет 57 %.\n",
    "\n",
    "*Recall* очень хорошо себя показывает в задачах, где важно найти как можно больше объектов, принадлежащих к классу 1.\n",
    "\n",
    "Например, это различные задачи, в которых мы пытаемся определить наличие эффекта, который может привести к серьёзным последствиям. Это могут быть те же диагностические задачи или задачи, в которых мы прогнозируем вероятность отказа устройства, от работы которого зависят человеческие жизни.\n",
    "\n",
    "Предельный случай (когда *recall* равен 1) означает, что модель нашла все объекты класса 1, например всех действительно больных пациентов. Однако метрика ничего не скажет о том, с какой точностью мы это сделали.\n",
    "\n",
    "**Примечание.** *Важно понимать, что данный вывод справедлив только для выборки, на которой мы оцениваем метрику, то есть это не означает, что модель вовсе не может допустить ложноотрицательных исходов. Однако чем больше выборка, на которой мы тестируем алгоритм, тем данный вывод будет ближе к истине.*\n",
    "\n",
    "Метрики *precision* и *recall* не зависят от сбалансированности классов и в совокупности дают довольно исчерпывающее представление о классификаторе. Однако на практике часто бывает так, **что увеличение одной из метрик может привести к уменьшению другой.**\n",
    "\n",
    "Концентрация только на одной метрике (*precision* или *recall*) без учёта второй — сомнительная идея.\n",
    "\n",
    "В битве за максимум *precision* для класса 1 побеждает модель, которая всегда будет говорить «нет». У неё вообще не будет ложноположительных срабатываний.\n",
    "\n",
    "В битве за максимум *recall* для класса 1 побеждает модель, которая всегда будет говорить «да». Она охватит все наблюдения класса 1, и у неё не будет ложноотрицательных срабатываний. \n",
    "\n",
    "В реальности необходимо балансировать между двумя этими метриками.\n",
    "\n",
    "    Классическим примером является задача определения оттока клиентов.\n",
    "\n",
    "    Очевидно, что мы хотим найти как можно больше клиентов, которые потенциально могут уйти от нас. Чтобы повысить их лояльность, мы планируем использовать ресурсы колл-центра. Однако они ограничены и мы не можем звонить всем клиентам. Определив стратегию и ресурс для удержания клиентов, мы можем подобрать нужные пороги по precision и recall. Например, можно сосредоточиться на удержании только высокодоходных клиентов или тех, кто уйдёт с большей вероятностью. \n",
    "\n",
    "5. В таком случае нам подойдёт следующая метрика.\n",
    "\n",
    "*$F_{betta}$ **(F-мера)** — это **взвешенное среднее гармоническое** между *precision* и *recall*:*\n",
    "\n",
    "![](./image/ML_3_13.png)\n",
    "\n",
    "*где $betta$ — это вес *precision* в метрике: чем больше $betta$, тем больше вклад.*\n",
    "\n",
    "В частном случае, когда $betta=1$, мы получаем равный вклад для *precision* и *recall*, а формула будет выражать простое среднее гармоническое, или метрику $F_1$ ($F_1$-мера):\n",
    "\n",
    "![](./image/ML_3_14.png)\n",
    "\n",
    "Рассчитаем метрику $F_1$ для нашего примера:\n",
    "\n",
    "![](./image/ML_3_15.png)\n",
    "\n",
    "**В чём преимущество $F_1$-меры?**\n",
    "\n",
    "*Метрика равна своему максимуму (1), если и precision, и recall равны 1 (то есть когда отсутствуют как ложноположительные, так и ложноотрицательные срабатывания). Но если хотя бы одна из метрик будет близка к 0, то и $F-мера$ будет близка к 0.*\n",
    "\n",
    "Несмотря на отсутствие бизнес-интерпретации, метрика $F_1$ является довольно распространённой и используется в задачах, где необходимо выбрать модель, которая балансирует между precision и recall.\n",
    "\n",
    "    Например, если цена дополнительной диагностики заболевания очень высока, то есть ложных срабатываний должно быть минимум, но при этом нам важно охватить как можно больше больных пациентов. \n",
    "\n",
    "**Примечание.** *Ещё одно небольшое, но очень важное замечание: все суждения, которые мы привели по отношению к precision, recall и $F$-мере, относятся только к классу 1, так как эти метрики по умолчанию считаются для класса 1. Для решения большинства задач знания о значении этих метрик для класса 1 более чем достаточно, так как обычно нас интересует именно наличие некоторого эффекта.*\n",
    "\n",
    "*Однако если вам по каким-то причинам необходимо рассчитать precision, recall и $F$-меру для класса 0, для этого достаточно сделать перекодировку классов — поменять их обозначения местами или (при расчёте метрик с помощью библиотеки sklearn) изменить значение специального параметра pos_label на 0.*\n",
    "\n",
    "Давайте обобщим всё вышесказанное в виде таблицы:\n",
    "\n",
    "![](./image/ML_3_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>РАСЧЁТ МЕТРИК НА PYTHON\n",
    "\n",
    "Код и результаты находятся в `Логическая_регресия.ipynb/Метрики классификации`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>ДОСТОИНСТВА И НЕДОСТАТКИ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ \n",
    "\n",
    "Давайте обобщим всё вышесказанное и приведём достоинства и недостатки логистической регрессии.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@happy-icon.png)\n",
    "\n",
    "* Простой, интерпретируемый, но в то же время эффективный алгоритм. Благодаря этому он очень популярен в мире машинного обучения.\n",
    "* Поиск параметров линейный или квадратичный (в зависимости от метода оптимизации), то есть ресурсозатратность алгоритма очень низкая.\n",
    "* Не требует подбора внешних параметров (гиперпараметров), так как практически не зависит от них.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@sad-icon.png)\n",
    "\n",
    "* Алгоритм работает хорошо, только когда классы линейно разделимы, что в реальных задачах бывает очень редко. Поэтому обычно данная модель используется в качестве baseline.\n",
    "\n",
    "В завершение изучения логистической регрессии можно добавить, что недостаток с линейной разделимостью классов можно побороть с помощью введения полиномиальных признаков, тем самым снизив смещение модели. Тогда логистическая регрессия вместо разделяющей плоскости будет означать выгнутую разделяющую поверхность сложной структуры.\n",
    "\n",
    "Однако мы знаем, что с этим трюком стоит быть аккуратным, так как можно получить переобученную модель. Поэтому в комбинации с полиномиальными признаками стоит подобрать наилучший параметр регуляризации.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-3_18.png)\n",
    "\n",
    "На рисунке выше изображены три различные модели:\n",
    "\n",
    "* первая — простая логистическая регрессия;\n",
    "* вторая — логистическая регрессия, обученная на полиномиальных признаках второй степени;\n",
    "* третья — логистическая регрессия, обученная на полиномиальных признаках десятой степени.\n",
    "\n",
    "Видно, что первая модель обладает низким качеством и не обобщает общей зависимости (у неё высокое смещение). Третья же, напротив, идеально выделяет каждое наблюдение в правильный класс, но является переобученной и также не отражает общей зависимости (у неё высокий разброс). Оптимальной моделью является вторая, которая не подстраивается под индивидуальные наблюдения и отражает общую зависимость в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ✍ Ранее мы с вами рассмотрели основы бинарной классификации. Но что делать, когда классов, на которые необходимо разделить данные, больше 2? Например, классификация автомобилей по различным маркам или определение национальности по фотографии и т. д.\n",
    "\n",
    "В таком случае используется очень простой подход, который называется **«один против всех» (one-vs-over)**.\n",
    "\n",
    "**Идея** *этого подхода очень простая. Если у нас есть $k$ различных классов ($k>2$), давайте обучим $k$ классификаторов, каждый из которых будет предсказывать вероятности принадлежности каждого объекта к определённому классу.*\n",
    "\n",
    "Например, у нас есть три класса, обозначенные как 0, 1 и 2. Тогда мы обучаем три классификатора: первый из них учится отличать класс 0 от классов 1 и 2, второй — класс 1 от классов 0 и 2, а третий — класс 2 от классов 1 и 0. Таким образом, класс, на который «заточен» классификатор, мы обозначаем как 1, а остальные классы — как 0.\n",
    "\n",
    "Когда каждая из трёх моделей сделает предсказание вероятностей для объекта, итоговый классификатор будет выдавать класс, который соответствует самой «уверенной» модели.\n",
    "\n",
    "Схематично это можно представить следующим образом:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_4_1.png)\n",
    "\n",
    "Если мы используем в качестве классификатора логистическую регрессию и количество факторов равно двум ($x_1$ и $x_2$), то можно изобразить тепловую карту вероятностей принадлежности к каждому из классов в каждой точке пространства, а также разделяющие плоскости, которые образуются при пороге вероятности в 0.5.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-3_10.png)\n",
    "\n",
    "На тепловых картах каждый класс обозначен своим цветом: 0 — зелёным, 1 — жёлтым, 2 — синим. Чем ярче цвет, тем выше вероятность принадлежности к каждому к классу в этой области пространства.\n",
    "\n",
    "В результате у нас получится три различных пространства вероятностей, что-то вроде трёх параллельных реальностей. Чтобы собрать всё это воедино, мы выбираем в каждой точке пространства максимум из вероятностей. Получим следующую картину:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-3_11.png)\n",
    "\n",
    "Модель логистической регрессии легко обобщается на случай мультиклассовой классификации. Пусть мы построили несколько разделяющих плоскостей с различными наборами параметров $k$, где $k$ — номер классификатора. То есть имеем $K$ разделяющих плоскостей:\n",
    "\n",
    "![](./image/ML_3_17.png)\n",
    "\n",
    "Чтобы преобразовать результат каждой из построенных моделей в вероятности в логистической регрессии, используется функция softmax — многомерный аналог сигмоиды:\n",
    "\n",
    "![](./image/ML_3_18.png)\n",
    "\n",
    "Данная функция выдаёт нормированные вероятности, то есть в сумме для всех классов вероятность будет равна 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>МУЛЬТИКЛАССОВАЯ КЛАССИФИКАЦИЯ НА PYTHON\n",
    "\n",
    "Код и результаты находятся в `Мультиклассовая_классификация.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ✍ В этом юните мы познакомимся с ещё одним семейством моделей машинного обучения — деревьями решений. Для начала поговорим о том, что такое дерево решений и как с его помощью решают задачу классификации.\n",
    "\n",
    "    Деревья решений являются одним из наиболее понятных человеку и в то же время мощных алгоритмов принятия решений. К тому же на их основе строятся самые эффективные ансамблевые модели машинного обучения, такие как случайный лес, о котором мы поговорим далее.\n",
    "\n",
    "    Алгоритмы на основе деревьев решений могут использоваться как для решения задач классификации, так и для регрессии. В этом модуле мы разберём задачу классификации, а в дальнейшем, когда будем разбирать математическую составляющую алгоритмов, поговорим о том, как научить дерево решать задачу регрессии.\n",
    "\n",
    "`Если коротко, решающее дерево предсказывает значение целевой переменной с помощью применения последовательности простых решающих правил. Этот процесс в некотором смысле согласуется с естественным для человека процессом принятия решений.`\n",
    "\n",
    "### <center>ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ДЕРЕВЕ РЕШЕНИЙ\n",
    "\n",
    "Начнём сразу с примера.\n",
    "\n",
    "Представьте, что у вас есть автомобиль, который вы решили застраховать. Вы приходите в страховую компанию, где вам дают заполнить анкету. По этой анкете сотрудник страховой компании будет принимать решение, стоит ли выдавать вам страховку.\n",
    "\n",
    "Сотрудник в свою очередь будет руководствоваться примерно следующим регламентом:\n",
    "\n",
    "* Если возраст владельца > 40 лет, то:\n",
    "  * Если место эксплуатации автомобиля — город, то:\n",
    "    * Если стаж > 10 лет, то:\n",
    "      * Застраховать.\n",
    "    * Если стаж < 10 лет, то:\n",
    "      * Не страховать.\n",
    "  * Если место эксплуатации автомобиля — сельская местность, то:\n",
    "    * Застраховать.\n",
    "* Если возраст владельца ≤ 40 лет, то:\n",
    "  * Если аварий не было зафиксировано, то:\n",
    "    * Застраховать.\n",
    "  * Если были аварии, то:\n",
    "    * Если тип автомобиля — минивэн, то:\n",
    "      * Застраховать.\n",
    "    * Если тип автомобиля — спорткар, то:\n",
    "      * Не страховать.\n",
    "\n",
    "То есть сотрудник при принятии решения использует информацию, предоставленную вами в анкете, и подает её на вход вложенного условного оператора.\n",
    "\n",
    "Для простоты восприятия можно представить такой подход визуально в виде следующего дерева:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_6_1.png)\n",
    "\n",
    "Аналогичным образом работает и алгоритм машинного обучения под названием **«дерево решений» (Decision Tree)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если дерево уже обучено, то есть уже сформулированы условия в прямоугольниках, то, когда в страховую компанию придёт новый автовладелец, сотруднику будет достаточно прогнать данные клиента через дерево решений и таким образом принять решение, то есть произвести классификацию.\n",
    "\n",
    "    Вот ещё один пример дерева решений. Большинство из нас когда-нибудь играли в игру «Слова на лбу» или «Тарантинки». На лоб каждого из игроков приклеивается бумажка с написанным на ней словом. Игрок может задавать другим игрокам вопросы о загаданном ему предмете/животном/человеке и т. д. Другие игроки могут отвечать на вопросы только «Да» и «Нет». Цель — за минимальное количество вопросов догадаться, о чём идёт речь.\n",
    "\n",
    "Логика «если …, то …» используется людьми повседневно и поэтому интуитивно понятна каждому из нас. На основании этих рассуждений можно построить мощный алгоритм машинного обучения.\n",
    "\n",
    "Деревья решений находят своё применение во множестве прикладных задач.\n",
    "\n",
    "**Успешнее всего деревья применяют в следующих областях:**\n",
    "\n",
    "* **Банковское дело.** Оценка кредитоспособности клиентов банка при выдаче кредитов.\n",
    "* **Промышленность.** Контроль качества продукции (обнаружение дефектов в готовых товарах), испытания без нарушений (например, проверка качества сварки) и т. п.\n",
    "* **Медицина.** Диагностика заболеваний разной сложности.\n",
    "* **Молекулярная биология.** Анализ строения аминокислот.\n",
    "* **Торговля.** Классификация клиентов и товара."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдём к формальной части. Нам важно уже сейчас познакомиться с терминологией деревьев решений, чтобы понять общий принцип их обучения.\n",
    "\n",
    "Пусть у нас есть всё та же матрица наблюдений X, в которой содержатся наблюдения и характеризующие их признаки (привычный нам DataFrame), и правильные ответы y — метки классов. \n",
    "\n",
    "Дадим определение дереву решений и его составляющим ↓\n",
    "\n",
    "Формально структура дерева решений — это **связный ациклический граф**. Что это значит?\n",
    "\n",
    "**Граф** *— это абстрактная топологическая модель, которая состоит из вершин и соединяющих их рёбер.*\n",
    "\n",
    "**Связный граф** *— это граф, в котором между любой парой существует направленная связь.*\n",
    "\n",
    "**Ациклический граф** *— это граф, в котором отсутствуют циклы, то есть в графе не существует такого пути, по которому можно вернуться в начальную вершину.*\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_6_2.png)\n",
    "\n",
    "**Примечание.** *Рекомендуем вам запомнить данное лаконичное определение дерева — так вы сможете показать свой уровень знаний перед будущим работодателем.*\n",
    "\n",
    "В дереве решений можно выделить **три типа вершин:**\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_6_3.png)\n",
    "\n",
    "* **Корневая вершина (root node)** — то, откуда всё начинается. Это первый и самый главный вопрос, который дерево задаёт объекту. В примере со страхованием это был вопрос «Возраст автовладельца > 40».\n",
    "* **Внутренние вершины (intermediate nodes)** — это дополнительные уточняющие вопросы, которые дерево задаёт объекту. \n",
    "* **Листья (leafs)** — конечные вершины дерева. Это вершины, в которых содержится конечный «ответ» — класс объекта.\n",
    "\n",
    "*Максимально возможная длина от корня до самых дальних листьев (не включая корневую) называется **максимальной глубиной дерева (max depth)**.*\n",
    "\n",
    "Во внутренней или корневой вершине признак проверяется на некий логический критерий, по результатам которого мы движемся всё глубже по дереву. Например, «Количество кредитов $<=$ 1». \n",
    "\n",
    "*Логический критерий, который находится в каждой вершине, называется **предикатом**, или **решающим правилом**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле все предикаты — это просто взятие порога по значению какого-то признака. Формально это записывается следующим образом:\n",
    "\n",
    "![](./image/ML_3_19.png)\n",
    "\n",
    "Предикат вершины дерева $B_v$ (где $v$ — это номер вершины) равен 1 («Да»), если признак $x_j$ меньше либо равен значению $t$, и 0 («Нет») — в противном случае. Функция $I$ с квадратными скобками — это уже знакомая нам индикаторная функция: она равна 1, если условие внутри скобок выполняется, и 0 — в противном случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание.** *В зависимости от реализации предикат может быть с условием $<=$ или $>=$. В реализации sklearn используется условие $<=$. Но вы можете встретить другую формулировку предикатов в иных реализациях или в литературе.*\n",
    "\n",
    "Если результат предиката равен 1, то мы переходим по левой ветви дерева к следующему узлу, в противном случае — по правой ветви дерева к следующему узлу.\n",
    "\n",
    "**А что насчёт геометрии?**\n",
    "\n",
    "Каждый новый вопрос дерева решений при его обучении разбивает пространство признаков на две части: в первую часть отправляются наблюдения, для которых предикат истинен, а во вторую — для которых он ложен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "\n",
    "Посмотрим, как это будет выглядеть, на примере. \n",
    "\n",
    "Вам уже знакома задача классификации про ирисы. **Ирисы Фишера** — это задача, на которой Рональд Фишер ещё в 1936 году (почти 100 лет назад!) продемонстрировал работу алгоритма, разделяющего ирисы на сорта в зависимости от параметров долей околоцветника.\n",
    "\n",
    "Пусть у нас есть следующие признаки:\n",
    "\n",
    "* длина внутренней доли околоцветника (англ. *petal length*);\n",
    "* ширина внутренней доли околоцветника (англ. *petal width*).\n",
    "\n",
    "На основании этих двух признаков требуется разделить ирисы на три сорта:\n",
    "\n",
    "* ирис щетинистый (*Iris Setosa*);\n",
    "* ирис виргинский (*Iris virginica*);\n",
    "* ирис разноцветный (*Iris versicolor*).\n",
    "\n",
    "Пусть мы обучили на этих данных дерево решений с максимальной глубиной 2. Оно получилось вот таким:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-5_4.png)\n",
    "\n",
    "В каждом блоке указаны следующие данные:\n",
    "\n",
    "* Предикат $I[x_j<=t]$ — условие, по которому выборка делится на две части: на ту, для которой условие выполняется, и ту, для которой не выполняется.\n",
    "* *gini* — критерий информативности Джини, о котором мы поговорим чуть позже.\n",
    "* *samples* — количество объектов, которые мы проверяем на данном шаге.\n",
    "* *value* — распределение по классам для объектов, которые мы проверяем на данном шаге: например `value=[0, 50, 50]` означает, что на текущем этапе разделения в выборке находится 0 объектов класса setosa и по 50 объектов классов *versicolor* и *virginica*.\n",
    "* *class* — класс, который мы присваиваем, если завершим выполнение алгоритма на данном шаге.\n",
    "\n",
    "А вот так будет выглядеть наш процесс разделения цветов на классы:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-5_5.png)\n",
    "\n",
    "Как происходит построение разделяющих плоскостей?\n",
    "\n",
    "* **Глубина дерева = 0.**\n",
    "\n",
    "Дерево задаёт первый вопрос: $petal length <= 2.45$. Это выражение соответствует вертикальной прямой, которая делит пространство на две части по признаку *petal length*.\n",
    "\n",
    "  * В левую часть пространства попали 50 наблюдений. Это только жёлтые точки пространства — цветы setosa. Значит, дальнейшее разделение не имеет смысла.\n",
    "\n",
    "  * В правую часть пространства попали 100 наблюдений. Это и синие, и зелёные объекты классов *versicolor* и *virginica*. Значит, нужно попробовать задать ещё одно решающее правило.\n",
    "\n",
    "* **Глубина дерева = 1.**\n",
    "\n",
    "Дерево задаёт второй вопрос: $petal width <= 1.75$. Это выражение соответствует горизонтальной прямой, которая делит оставшееся после прошлого разделения пространство на две части по признаку *petal width*.\n",
    "\n",
    "  * В нижнюю (синюю) часть этого пространства попали 54 наблюдения. Из них 49 цветов класса *versicolor* и 5 цветов класса *virginica*.\n",
    "\n",
    "    Максимальная глубина достигнута. В полученной части пространства преобладает класс *versicolor*, значит все наблюдения, которые находятся в этой части, дерево будет относить к классу *versicolor*.\n",
    "\n",
    "  * В верхнюю (зелёную) часть этого пространства попали 46 наблюдений. Из них 1 цветок класса *versicolor* и 45 цветов класса *virginica*.\n",
    "\n",
    "    Максимальная глубина достигнута. В полученной части пространства преобладает класс *virginica*, значит все наблюдения, которые находятся в этой части, дерево будет относить к классу *virginica*.\n",
    "\n",
    "Отметим, что деление пространства можно продолжать до тех пор, пока пространство не будет разделено так, чтобы верно выделить каждый из классов. \n",
    "\n",
    "Кстати, для каждой области можно подсчитать вероятность каждого из классов. Это просто отношение количества объектов $k$-класса, которые попали в лист дерева, к общему количеству объектов в листе.\n",
    "\n",
    "Например, для синей области вероятности будут равны:\n",
    "\n",
    "![](./image/ML_3_20.png)\n",
    "\n",
    "_______\n",
    "\n",
    "Теперь, когда мы разобрались с терминологией и геометрией, давайте поговорим о том, как строится решающее дерево."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>ПРОЦЕСС ПОСТРОЕНИЯ ДЕРЕВА РЕШЕНИЙ\n",
    "\n",
    "    ✍ Существует множество стратегий построения деревьев решений. Мы рассмотрим стратегию, реализованную в библиотеке sklearn, — алгоритм CART (Classification and Regression Tree), который предназначен для построения бинарных деревьев решений (деревьев, у которых каждая вершина связана с двумя другими вершинами нижнего уровня). Данный алгоритм, как следует из его названия, предназначен для решения задач классификации и регрессии.\n",
    "\n",
    "Внимательный студент уже заметил, что построение дерева решений можно описать рекурсией. Каждая вершина дерева порождает две других вершины, а они в свою очередь порождают новые вершины, и так происходит до тех пор, пока не выполнится некоторый критерий остановки, например в вершине не останутся только наблюдения определённого класса.\n",
    "\n",
    "**Примечание.** *Если вы забыли, что такое рекурсия, рекомендуем вам вернуться к модулю по продвинутому использованию функций и активировать рекурсивное мышление, оно нам понадобится.*\n",
    "\n",
    "Пусть у нас есть матрица наблюдений X и столбец с ответами — метками классов y. На основе примеров и ответов мы хотим построить дерево решений, которое будет производить классификацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, псевдокод рекурсивной функции для построения решающего дерева будет выглядеть следующим образом (запускать код не нужно, так как он является абстрактным):\n",
    "\n",
    "```\n",
    "def build_decision_tree(X, y):\n",
    "    node = Node()\n",
    "    if stopping_criterion(X, y) is True:\n",
    "        node = create_leaf_with_prediction(y)\n",
    "\treturn node\n",
    "    else:\n",
    "        X_left, y_left, X_right, y_right = best_split(X, y)\n",
    "        node.left = build_decision_tree(X_left, y_left)\n",
    "        node.right = build_decision_tree(X_right, y_right)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберёмся, как работает алгоритм:\n",
    "\n",
    "1. Создать новую вершину *node*.\n",
    "\n",
    "На первой итерации это будет корневая вершина. На последующих это будут внутренние вершины.\n",
    "\n",
    "2. Проверить некоторый критерий остановки `stop_criterion()`.\n",
    "\n",
    "Например, критерием остановки может быть следующее условие: все объекты, которые попали в вершину, — это объекты одного и того же класса.\n",
    "\n",
    "Или достигнута максимальная глубина дерева (max_depth), например 5. Это значит, что дерево не будет продолжать делиться, если глубина уже равна 5.\n",
    "\n",
    "Другой критерий: число наблюдений в листе (в sklearn этот параметр обозначен как min_samples_leaf) меньше заданного, например 7. Это значит, что при выполнении такого условия дерево продолжит делиться в том случае, если решающее правило выполняется как минимум для 7 наблюдений.\n",
    "\n",
    "**2.1. Если условие остановки выполняется:**\n",
    "\n",
    "Проверить, какой класс преобладает в текущей вершине. Превратить текущую вершину дерева в лист, где всем наблюдениям, которые попали в эту вершину, присвоить метку преобладающего класса.\n",
    "\n",
    "Прекратить построение дерева, вернув из алгоритма полученный лист.\n",
    "\n",
    "**2.2. Если условие остановки не выполняется:**\n",
    "\n",
    "Среди всех возможных предикатов $B_v(x,t) = I[x_j<=t]$ найти такой, который обеспечивает разбиение выборки наилучшим образом.\n",
    "\n",
    "То есть нужно найти такой признак $x_j$ и пороговое значение $t$, при которых достигается максимум некоторой информативности (существуют разные меры информативности, о них поговорим ниже). Назовём эту часть алгоритма некоторой абстрактной функцией `best_split()`.\n",
    "\n",
    "Например, в нашем примере с ирисами это был предикат $Petal.Length <= 2.45$. Он обеспечил наилучшее разделение пространства на две части.\n",
    "\n",
    "В результате разбиения будут созданы два набора данных:\n",
    "\n",
    "* `X_left, y_left` (левый), для которого выполняется условие $x_j<=t$;\n",
    "* `X_right, y_right` (правый), для которого условие не выполняется.\n",
    "\n",
    "Создаются две новые вершины: левая и правая, в каждую из которых отправляется соответствующий набор данных.\n",
    "\n",
    "То есть происходит рекурсивный вызов функции `build_decision_tree()`, и для каждой новой вершины алгоритм повторяется вновь с новым набором данных.\n",
    "\n",
    "**Примечание.** *Вершина дерева node задаёт целое поддерево идущих за ним вершин, если такие имеются, а не только саму вершину.*\n",
    "\n",
    "Центральный момент в построении дерева решений по обучающему набору данных — найти такой предикат $B_v(x,t) = I[x_j<=t]$, который обеспечит наилучшее разбиение выборки на классы. \n",
    "\n",
    "Как дерево определяет, какой вопрос нужно задать в каждой из вершин? \n",
    "\n",
    "Например, в задаче кредитного скоринга мы можем задавать множество различных вопросов в разной последовательности. Предикаты $B_0$ в первой вершине могут быть различными:\n",
    "\n",
    "* возраст заёмщика $<=$ 25 лет,\n",
    "* возраст заёмщика $<=$ 40 лет,\n",
    "* размер кредита $<=$ 1000 $,\n",
    "* наличие детей $<=$ 0.5 (если наличие детей — бинарный категориальный признак: 1 — есть дети, 0 — нет детей),\n",
    "* и так далее.\n",
    "\n",
    "Видно, что на место $x_j и $t$ можно подставить любой признак и порог соответственно.\n",
    "\n",
    "Признак $x_j$ и его пороговое значение $t$ в каждой из вершин и есть внутренние параметры дерева решений, которые мы пытаемся отыскать. Это аналог коэффициентов уравнения линейной и логистической регрессий. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>ПОИСК ПАРАМЕТРОВ ДЕРЕВА РЕШЕНИЙ\n",
    "\n",
    "    → Обратите внимание, что внутренние параметры дерева решений кардинально отличаются от линейных моделей.\n",
    "\n",
    "В линейных моделях мы пытались найти такие коэффициенты в уравнениях, при которых наблюдался минимум функции потерь.\n",
    "\n",
    "В деревьях же мы пытаемся выбрать такие признаки $x_j$ и их пороговые значения $t$, при которых произойдёт разделение набора на две части по какому-то критерию наилучшим образом. В нашем псевдокоде этот процесс организован в виде функции `best_split()`.\n",
    "\n",
    "    → Важно понимать, что дерево решений — это топологический алгоритм, а не аналитический, то есть структуру дерева не получится описать в виде формулы, как те же линейные модели. Поэтому про стандартные методы оптимизации, такие как градиентный спуск или тем более метод наименьших квадратов, можно забыть. \n",
    "\n",
    "Чтобы интуитивно понять, как организуется поиск параметров, вспомним про игру «Слова на лбу».\n",
    "\n",
    "    Пусть один человек загадывает знаменитость, а второй пытается отгадать, задавая только вопросы, на которые можно ответить «Да» или «Нет» (опустим варианты «не знаю» и «не могу сказать»).\n",
    "\n",
    "    Какой вопрос отгадывающий задаст первым делом? Конечно, такой, который лучше всего уменьшит количество оставшихся вариантов.\n",
    "\n",
    "    К примеру, вопрос «Это Анджелина Джоли?» в случае отрицательного ответа оставит более 7.5 миллиардов вариантов для дальнейшего перебора (строго говоря, поменьше, ведь не каждый человек — знаменитость, но всё равно немало), а вот вопрос «Это женщина?» отсечёт уже около половины знаменитостей.\n",
    "\n",
    "    То есть, признак пол намного лучше разделяет выборку людей, чем признак это Анджелина Джоли, национальность — испанец или любит футбол.\n",
    "\n",
    "    Интуитивно это соответствует уменьшению некоторой неопределённости, или, иначе говоря, повышению прироста информативности.\n",
    "\n",
    "В случае «угадайки» знаменитостей критериев отбора может быть бесчисленное количество. Но когда мы работаем с набором данных, у нас ограниченное количество признаков и для них есть ограниченное количество порогов. Тогда мы можем полным перебором найти такую комбинацию $j$ и $t$, которая обеспечит наилучшее уменьшение неопределённости.\n",
    "\n",
    "Неопределённость можно измерять различными способами, в деревьях решений для этого используются **энтропия Шеннона** и **критерий Джини**. Мы подробно обсудим их реализацию в модулях по математике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>ДЕРЕВЬЯ РЕШЕНИЙ В SKLEARN\n",
    "\n",
    "_______\n",
    "Подробный разбор в `./Деревья_решений.ipynb`\n",
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>ДОСТОИНСТВА И НЕДОСТАТКИ ДЕРЕВЬЕВ РЕШЕНИЙ\n",
    "\n",
    "Обобщим всё вышесказанное, выделив основные достоинства и недостатки деревьев решений.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@happy-icon.png)\n",
    "\n",
    "* Дерево решений не требует нормализации/стандартизации данных.\n",
    "* Наличие пропусков не оказывает существенного влияния на построение дерева.\n",
    "* За счёт своей простоты модель деревьев решений интуитивно понятна и легко объяснима даже людям, не разбирающимся в методе.\n",
    "* Приятный побочный эффект построения дерева решений — получение значимости признаков. Однако коэффициенты значимости целиком и полностью зависят от сложности дерева.\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@sad-icon.png)\n",
    "\n",
    "* В силу дискретной топологической структуры дерево не дифференцируется по параметрам: стандартные алгоритмы поиска параметров, такие как градиентный спуск, не работают. Приходится использовать полный перебор.\n",
    "\n",
    "**Примечание.** *Количество перебираемых вариантов можно сократить, используя методы динамического программирования (их изучение не входит в рамки нашего курса).*\n",
    "\n",
    "* Так как метод является жадным, он долго обучается из-за полного перебора. Требует затрат больших вычислительных мощностей (по сравнению с другими алгоритмами). Особенно это ощутимо при большом количестве признаков на глубоких деревьях.\n",
    "* Очень сильная склонность к переобучению. Необходим подбор внешних параметров: max_depth, min_sample_leaf и другие (о том, как организовать этот подбор, мы поговорим в отдельном модуле).\n",
    "* Небольшое изменение в данных может заметно повлиять на структуру дерева.\n",
    "* При работе с непрерывными числовыми признаками дерево делит их на категории и теряет информацию. Лучше всего дерево работает, если перевести числовые признаки в категориальные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "    ✍ Одна модель хорошо, а много — лучше! С этой фразы мы с вами приоткрываем дверь в удивительный мир ансамблевых моделей.\n",
    "\n",
    "**Ансамблевые модели** или просто **ансамбли (ensembles)** *— это метод машинного обучения, где несколько простых моделей (часто называемых «слабыми учениками») обучаются для решения одной и той же задачи и объединяются для получения лучших результатов.*\n",
    "\n",
    "`Необходимость использования ансамблей может возникнуть тогда, когда вы уже нашли хорошую модель и никак больше не можете повысить её качество. В этом случае можно перейти к более продвинутому методу: использовать не одну модель (пусть и очень хорошую), а ансамбли моделей.`\n",
    "\n",
    "Ансамбли — передовые алгоритмы для решения сложных задач машинного обучения. Сегодня они и нейронные сети являются главными конкурентами и дают наилучшие результаты, благодаря чему используются крупными компаниями в продакшене.\n",
    "\n",
    "    → В этом модуле мы коснёмся основой идеи использования ансамблей на примере бэггинга и посмотрим на его реализацию для решения задач классификации в библиотеке sklearn, а в дальнейшем разберёмся, что за математическая «магия» стоит за ансамблевыми методами.\n",
    "\n",
    "Говоря простыми словами, ансамбли — это объединение простых моделей в одного гиганта. Но объединять модели можно как угодно: например, взять тысячу разных логистических регрессий, а затем на их предсказаниях построить дерево решений, линейную регрессию или вообще нейронную сеть. А можно обучить сотню деревьев решений — построить целый лес, а для предсказания взять среднее. Вариаций объединения может быть сколько угодно.\n",
    "\n",
    "Существует **три проверенных способа построения ансамблей:**\n",
    "\n",
    "* **Бэггинг** — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "* **Бустинг** — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "* **Стекинг** — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение.\n",
    "\n",
    "Об ансамблях типов бустинг и стекинг мы поговорим чуть позже в модуле, посвящённом продвинутым методам машинного обучения. В этом модуле разберёмся, что из себя представляет бэггинг и причём тут деревья решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>БЭГГИНГ. ОСНОВНЫЕ ИДЕИ\n",
    "\n",
    "    В 1906 г. в городе Плимут (Великобритания) на сельской ярмарке был проведён эксперимент. Фрэнсис Гальтон в качестве развлечения посетителей ярмарки предложил им на глаз оценить вес выставленного на всеобщее обозрение быка. За правильные ответы организаторы шоу обещали призы. В результате в голосовании приняли участие около 800 человек — как заядлых фермеров, так и людей, далёких от скотоводческих дел. Собрав после этой ярмарки все результаты, Гальтон высчитал среднее арифметическое значение для всей выборки — 1197 фунтов. Реальный же вес быка оказался 1198 фунтов. Каким-то непостижимым образом разношерстная публика дала ответ, максимально приближенный к реальному показателю. То есть ответ публики был точнее, чем ответ отдельно взятого эксперта, например мясника или скотовода.\n",
    "\n",
    "По схожему принципу «голосования толпы» и работает бэггинг.\n",
    "\n",
    "**Бэггинг (bagging)** *— это алгоритм построения ансамбля путём параллельного обучения множества независимых друг от друга моделей.*\n",
    "\n",
    "В основе алгоритма лежит статистический метод, который называется **бутстрэпом (bootstrap)**. Идея бутстрэпа заключается в генерации $k$ выборок размера $n$ (бутстрэп-выборок) из исходного набора данных размера $m$ путём случайного выбора элементов с повторениями в каждом из наблюдений. \n",
    "\n",
    "Схематично работу метода можно представить следующим образом:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_7_1.png)\n",
    "\n",
    "Мы взяли исходный набор данных размером $m=12$ наблюдений и сгенерировали $k=3$ бутстрэп-выборки размером $n=8$ наблюдений. Обратите внимание, что наблюдения в выборках могут повторяться.\n",
    "\n",
    "В частном случае можно генерировать выборки того же размера, то есть $n=m$. Таким образом, мы из набора данных создаём $k$ датасетов из исходного того же размера.\n",
    "\n",
    "**Примечание.** *Такие бутстрэп-выборки часто используются для оценки различных статистических показателей (например, разброса или доверительного интервала). Если вычислять статистические оценки на нескольких независимых выборках, то мы можем оценить их разброс. Поиск большого количества независимых выборок сложен в силу того, что для этого требуется слишком много данных. Поэтому мы используем бутстрэп, чтобы создать несколько выборок.*\n",
    "\n",
    "Давайте обучим $k$ одинаковых моделей на каждой из сгенерированных выборок, сделаем предсказания, а затем усредним их. Так мы получим бэггинг.\n",
    "\n",
    "Схематично такой подход можно описать следующим образом:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_7_2.png)\n",
    "\n",
    "**Примечание.** *В случае классификации «усреднение» означает мажоритарное голосование (принцип большинства голосов). То есть объект относится к тому классу, за который проголосовало большинство алгоритмов.*\n",
    "\n",
    "Важно отметить, что в бэггинге в голосовании принимает участие модель одного вида. Эта модель называется **базовой моделью (base model)**. Нельзя обучить на половине сгенерированных наборов данных логистические регрессии, а на второй половине — деревья решений.\n",
    "\n",
    "Когда мы будем разбирать математическое обоснование ансамблей, мы докажем следующие два утверждения:\n",
    "\n",
    "* Смещение (*bias*) бэггинг-ансамбля не больше ($<=$) смещения одного алгоритма из этого ансамбля.\n",
    "* Однако разброс (*variance*) бэггинг-ансамбля в $k$ раз меньше, чем разброс одного алгоритма из ансамбля, где $k$ — количество алгоритмов в ансамбле.\n",
    "\n",
    "**Что это значит в переводе с математического?**\n",
    "\n",
    "*Теорема гарантирует, что средняя ошибка ансамбля, построенного по принципу бэггинга, не выше, чем средняя ошибка базовой модели, но при этом шанс переобучения алгоритма значительно ниже.*\n",
    "\n",
    "Это очень важно для моделей, склонных к переобучению, таких как глубокие деревья решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>СЛУЧАЙНЫЙ ЛЕС\n",
    "\n",
    "**Случайный лес (Random Forest)** *— это самая распространённая реализация бэггинга, основанная на использовании в качестве базовой модели дерева решений.*\n",
    "\n",
    "Помимо бутстрэпа, случайный лес использует метод случайных подпространств. Суть этого метода заключается в том, что каждая модель обучается не на всех признаках, а только на части из них. Такой подход позволяет уменьшить коррелированность между ответами деревьев и сделать их независимыми друг от друга.\n",
    "\n",
    "<H_3>Алгоритм построения случайного леса для задачи классификации<H_3>\n",
    "\n",
    "Пусть количество объектов в наборе данных равно N, а количество признаков — *M*. То есть размер набора данных — (*N, M*). Количество деревьев в лесу равно *K*. Тогда для обучения случайного леса необходимо выполнить следующие шаги:\n",
    "\n",
    "1. С помощью бутстрэпа создать K наборов данных размера (*N, M*).\n",
    "2. Для каждого сгенерированного набора данных применить метод случайных подпространств: выбрать *L < M* случайных признаков и получить K новых наборов данных размером (*N, L*).\n",
    "3. На каждом наборе данных обучить дерево решений.\n",
    "\n",
    "Когда поступят новые данные, нам нужно будет прогнать их через каждое дерево и объединить результаты отдельных деревьев мажоритарным голосованием или путём комбинирования вероятностей.\n",
    "\n",
    "Ниже приведена схема работы описанного алгоритма для решения задачи классификации. Для простоты лес состоит из четырёх деревьев (*K=4*).\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_7_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "Давайте посмотрим, как работает алгоритм случайного леса, на примере. \n",
    "\n",
    "Пусть у нас есть набор данных со следующими факторами:\n",
    "\n",
    "* $x_1$ — возраст,\n",
    "* $x_2$ — доход в тысячах рублей,\n",
    "* $x_3$ — группа крови.\n",
    "\n",
    "**Целевой признак** ($y$) — подвергался ли человек операции хотя бы раз в жизни (1 — да, 0 — нет).\n",
    "\n",
    "Тогда, если мы обучим алгоритм случайного леса на представленных данных, мы получим следующую картину:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_7_4.png)\n",
    "\n",
    "Количество объектов в наборе данных N=5, а количество факторов — M=3 (+1 целевой признак, его мы не включаем в размерность). Таким образом, размер таблицы наблюдений — (5, 3). Количество деревьев в лесу K=4.\n",
    "\n",
    "1. На первом этапе алгоритма мы формируем K=4 бутстрэп выборки размером (5, 3), выбирая из таблицы строки случайным образом с возможностью повторения.\n",
    "2. На втором этапе мы случайным образом выбираем L=2 признаков из каждой таблицы и получаем четыре выборки размером (5, 2).\n",
    "3. На третьем этапе мы обучаем K=4 деревьев решений, каждое на своей выборке.\n",
    "\n",
    "*Обратите внимание, что деревья, из которых состоит лес, могут быть различной глубины и структуры в зависимости от того, насколько просто была разделима поданная выборка.*\n",
    "\n",
    "Теперь, когда поступят новые данные, нам останется только подать их на вход каждого из деревьев, получить предсказания, а затем усреднить их путём мажоритарного голосования и получить ответ:\n",
    "\n",
    "![](./image/asset-v1_SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_7_5.png)\n",
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "### <center>СЛУЧАЙНЫЙ ЛЕС В SKLEARN\n",
    "\n",
    "Подробный разбор в `Введение_в_ансамбли_бэггинг._Случайный_лес.ipynb`\n",
    "_______"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
